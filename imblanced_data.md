# 解决样本不均衡的方法  
## 随机欠采样  
在数据较多的样本中进行下采样，使其与数据较少的类别数量相同。  
缺点：数据减少，会影响模型的特征学习能力和泛化能力  
优点：数据量减少，加速训练  
##过采样  
针对数据较少的类别，重复对其采样直到和其他样本数量相似  
优点：数据没有信息损失  
缺点：增加了过拟合的可能性  
## 基于聚类的过采样  
先对每种样本进行聚类，样本多的类别聚类中心数目较多，样本少的类较少。然后  
对聚类后的每个cluster进行降采样/过采样，使得每个cluster样本数量相似。  
## SMOTE  
* 由于过采样容易导致过拟合，所以提出了人造数据来减弱过采样带来的过拟合效果  
* 对于数据少的样本，随机找一个样本xi，先找到其临近的K个样本，然后从这几个临近  
  样本中随机选取一个yi，利用x = xi +(yi - xi) * &  生成一个新样本  
  可以看成是在两个样本中间点选择了一个点作为人工样本  
  
## 基于数据清洗的SMOTE  
* 实际问题中不但存在着样本不平衡问题，还伴随着不同类别的样本重叠问题。如果直接对  
  样本进行SMOTE采样，将会导致样本重叠加剧  
  采用Tomek Links方法对数据进行清洗。
* 对于一对样本（xi,xj），xi来自少量样本，xj来自大量样本。如果不存在一个xk使得：  
  d(xi,xk)<d(xi,xj) or d(xj,xk) <d(xi,xj),就是说没有一个样本在他们两个之间，  
  表示他们已经离得很近了，则在进行过采样时，把他们去掉。
## 集成算法  
### Bagging  
先生成不同的bootstrap训练集样本，然后将算法分别在每个样本集上进行训练，最后将预测  
结果进行融合，与Boosting不同的是，Bagging是允许有放回的数据采样  
优点：提高算法的稳定性和准确率，减弱过拟合  
缺点：在每个基础的分类器都能产生不错的结果的时候才能较好的工作  
### Bossting  
通过将弱分类器结合起来构成强分类器，在新一轮迭代中，新的分类器更加关注上一轮迭代中分  
错的样本，给这些样本赋予更大的权重。
